{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2486696",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise Image-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d5148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T16:47:37.010939Z",
     "start_time": "2023-04-26T16:47:37.006064Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import paths\n",
    "from datasets import VOCDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591da245-9ab5-43cc-8df2-17a5c31cd7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Image Captioning\n",
    "# Your first task will be to complete the inference code to generate captions for the given VOC dataset.\n",
    "from eval_captioning import extract_evaluate_write_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ce6b3-fd3d-481a-b164-e7731d3d385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Complete Caption Generation with Greedy Search\n",
    "\n",
    "# TODO: In file models/blip/blip_caption.py complete the methods generate and greedy_search. \n",
    "#       Generate and evaluate captions for the VOC dataset. You should get about 28% BLEU score. (2 points)\n",
    "\n",
    "extract_evaluate_write_captions(use_topk_sampling=False, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1455059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T16:47:37.256254Z",
     "start_time": "2023-04-26T16:47:37.192994Z"
    }
   },
   "outputs": [],
   "source": [
    "voc_path = Path(paths.CV_PATH_VOC)\n",
    "dataset = VOCDataset(voc_path, voc_path / \"ImageSets\" / \"Segmentation\" / \"val.txt\",\n",
    "                     load_captions=True)\n",
    "\n",
    "# load and show generated captions\n",
    "# todo update the path to match your experiment\n",
    "pred_captions_file = \"outputs/eval_captioning/2024_06_04_11_49_15/pred_captions.txt\"\n",
    "with open(pred_captions_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    pred_captions = f.readlines()\n",
    "    \n",
    "from PIL import Image\n",
    "for i in range(10):\n",
    "    data = dataset[i]\n",
    "    display(data[\"image\"])\n",
    "    print(f\"Pred caption: {pred_captions[i]}\")\n",
    "    print(f\"Reference caption: {data['caption']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eceaf88",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 1.2 Complete Caption Generation with Sampling\n",
    "# TODO: In file models/blip/blip_caption.py complete the method sampling. \n",
    "#       There, Top-K sampling instead of greedy search is used to select the next token when decoding. \n",
    "#       Evaluate again with Top-K sampling.\n",
    "#       You should get a lower BLEU score of about 7% for temperature τ = 1.0, and about 12% for τ = 0.7. \n",
    "#       Why do the results improve with lower temperature? (1 point)\n",
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=50, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0f95e-95e1-4eaf-b6c4-387268bfc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=50, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc3672-69b2-4378-a2e9-7d991ec300be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Prompt Engineering\n",
    "# TODO: Experiment with different prompts (the default prompt is “a picture of ”). \n",
    "#       Can you improve the BLEU score? Try at least 3 new settings.\n",
    "#       Note the prompt and the resulting BLEU score in a table for each setting. \n",
    "#       Add the table to your report. (1 point)\n",
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=50, temperature=0.7, prompt=\"a picture of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d59538-6ac5-4392-aadb-739f5d74abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=50, temperature=0.7, prompt=\"an image of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b42e97-6b01-407a-b88f-4486c780849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=50, temperature=0.7, prompt=\"a depiction of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470397a-c072-4318-a1e8-ca81b1b3afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=50, temperature=0.7, prompt=\"a figure of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4abaff-6d80-43dc-9e2f-4377b09548b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Student Hyperparameter Search\n",
    "# TODO: Experiment with different decoding parameters. (Top-K with different K and temperature or greedy decoding). \n",
    "#       Can you improve the BLEU score? Try at least 3 new settings.\n",
    "#       Note the hyperparameters, the prompt and the resulting BLEU score in a table for each setting.\n",
    "#       Add the table to your report. (1 point)\n",
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=50, temperature=0.7, prompt=\"a picture of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc759993-9da7-4b85-b5f9-3ff5848181cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=100, temperature=0.5, prompt=\"an image of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8138d2-90f0-45fe-a307-17f1def23f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=25, temperature=0.3, prompt=\"a depiction of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c959ffb9-9309-44df-a638-d0f23bb68ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=10, temperature=1.0, prompt=\"a figure of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f5607-a951-45e9-a06c-1cf9619216d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=75, temperature=1.0, prompt=\"a figure of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ae6ad-c6af-4f4c-9e6d-0d0501eef6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_evaluate_write_captions(use_topk_sampling=True, topk=100, temperature=0.3, prompt=\"an image of \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf535e-447d-4493-bd4f-e12731007a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f0cb7-6677-49ed-be36-1891dae6e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Image-Text Retrieval\n",
    "# Your second task will be to train and evaluate the retrieval head of the BLIP model.\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d87144-6833-4d58-ac2b-9bc8f4e69119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Complete Forward Pass and Evaluate\n",
    "#     Todo: Complete the forward pass in file models/blip/blip_retrieval.py. \n",
    "#           Evaluate your implementation with the provided checkpoint. \n",
    "#           You should get about 54% image-to-text R@1. (2 points)\n",
    "from eval_retrieval import eval_without_args    \n",
    "eval_without_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4778a-aa46-4933-95c0-98b66b697bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Complete Loss and Train from Scratch\n",
    "# Todo: Complete the loss computation in file train_retrieval.py function train_epoch. \n",
    "#       Train the retrieval projection layers from scratch (i.e. from random initialization).\n",
    "#       You should get about 43% image-to-text R@1. (1 point)\n",
    "from train_retrieval import train_retrieval_without_args\n",
    "train_retrieval_without_args(finetune=False, learning_rate=1e-3, weight_decay=1e-3, epochs=5, temperature=0.1)\n",
    "\n",
    "# Optional: start a tensorboard server tensorboard --logdir outputs --port 6006 and watch the experiment in the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb532e-65e3-4e24-8fa9-2a26e3ea6a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Finetune instead of Train from Scratch\n",
    "# Todo: Now, try finetuning the head instead with --finetune. \n",
    "#       Set learning rate to 1e-5, weight decay to 0 and train for 3 epochs. \n",
    "#       What score do you get and how can you explain the difference to the score when training from scratch? (1 point)\n",
    "#       Try different search queries. What do you observe?\n",
    "    \n",
    "train_retrieval_without_args(finetune=True, learning_rate=1e-5, weight_decay=1e-3, epochs=3, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19453f-6416-43d2-b50b-8df4c4c72565",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retrieval_without_args(finetune=True, learning_rate=1e-5, weight_decay=0, epochs=3, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64660e3d-13a2-4fd9-bea9-bd22f32c3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Student Hyperparameter Search\n",
    "# Todo: Experiment with different hyperparameters in the random initialization setting (i.e. without finetuning). \n",
    "#       Try at least 3 new hyperparameter settings.\n",
    "#       Note the hyperparameters and the resulting image-to-text R@1 score in a table for each setting. \n",
    "#       Can you improve over the baselines? Add the table to your report. (1 point)\n",
    "\n",
    "# train_retrieval_without_args(...)\n",
    "train_retrieval_without_args(finetune=False, learning_rate=1e-5, weight_decay=0, epochs=5, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed71f1-3a45-4043-89a3-b3a6ba0e1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retrieval_without_args(finetune=False, learning_rate=5e-1, weight_decay=1e-3, epochs=5, temperature=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a6218-41e1-4fbb-802a-1483465ace83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retrieval_without_args(finetune=False, learning_rate=5e-3, weight_decay=1e-4, epochs=8, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d1279-516e-4e6b-9b2f-179683701d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retrieval_without_args(finetune=False, learning_rate=1e-3, weight_decay=0, epochs=8, temperature=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d34c97-d8c7-4f61-a027-5f60e2b04b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_retrieval_without_args(finetune=False, learning_rate=1e-5, weight_decay=0, epochs=3, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de0659-9b4b-4875-815a-b58a82cfc8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize Top 10 results for a search query.\n",
    "\n",
    "from search_retrieval import get_top10\n",
    "\n",
    "search_query = \"a picture of a plane\"\n",
    "dict_top10 = get_top10(eval_ckpt=None, query = search_query)\n",
    "\n",
    "from PIL import Image\n",
    "for i in range(len(dict_top10[\"id\"])):\n",
    "    image_pil = Image.open(dict_top10[\"fname\"][i])\n",
    "    display(image_pil)\n",
    "    print(f\"Sim. Score: {dict_top10['sim'][i]}\")\n",
    "    print(f\"Caption: {dict_top10['caption'][i]}\")\n",
    "    #print(f\"Name: {dict_top10['name'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb8b9f-1e23-46cb-8784-13d8d8065b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
